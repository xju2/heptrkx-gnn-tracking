#!/usr/bin/env python
"""
Training GNN
"""

import tensorflow as tf

import os
import sys
import argparse
import glob
import re
import time
import random
import functools

import numpy as np
import sklearn.metrics


from graph_nets import utils_tf
from graph_nets import utils_np
import sonnet as snt

from heptrkx.dataset import graph
from heptrkx.nx_graph import get_model
from heptrkx.utils import load_yaml
# ckpt_name = 'checkpoint_{:05d}.ckpt'
ckpt_name = 'checkpoint'

prog_name = os.path.basename(sys.argv[0])

def eval_output(target, output):
    """
    target, output are graph-tuple from TF-GNN,
    each of them contains N=batch-size graphs
    """
    tdds = utils_np.graphs_tuple_to_data_dicts(target)
    odds = utils_np.graphs_tuple_to_data_dicts(output)

    test_target = []
    test_pred = []
    for td, od in zip(tdds, odds):
        test_target.append(np.squeeze(td['edges']))
        test_pred.append(np.squeeze(od['edges']))

    test_target = np.concatenate(test_target, axis=0)
    test_pred   = np.concatenate(test_pred,   axis=0)
    return test_pred, test_target


def compute_matrics(target, output, thresh=0.5):
    test_pred, test_target = eval_output(target, output)
    y_pred, y_true = (test_pred > thresh), (test_target > thresh)
    return sklearn.metrics.precision_score(y_true, y_pred), sklearn.metrics.recall_score(y_true, y_pred)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Train nx-graph with configurations')
    add_arg = parser.add_argument
    add_arg('config', help='configuration file')

    add_arg('--from-hitsgraph', help='from hits graph', action='store_true')
    add_arg('--hitsgraph-dir', help='histgraph directories',
        default='/project/projectdirs/m3443/usr/dtmurnane/doublets/high_fullsplit')
    args = parser.parse_args()

    all_config = load_yaml(args.config)
    config = all_config['gnn_training']


    # add ops to save and restore all the variables
    prod_name = config['prod_name']
    output_dir = os.path.join(config['output_dir'], prod_name)
    print("[{}] save models at {}".format(prog_name, output_dir))
    os.makedirs(output_dir, exist_ok=True)

    # files = glob.glob(output_dir+"/*.ckpt.meta")
    # last_iteration = 0 if len(files) < 1 else max([
    #     int(re.search('checkpoint_([0-9]*).ckpt.meta', os.path.basename(x)).group(1))
    #     for x in files
    # ])
    # print("[{}] last iteration: {}".format(prog_name, last_iteration))

    config_tr = config['parameters']
    log_every_seconds       = config_tr['time_lapse']
    global_batch_size = n_graphs   = config_tr['batch_size']   # need optimization
    num_training_iterations = config_tr['iterations']
    iter_per_job            = 2500 if 'iter_per_job' not in config_tr else config_tr['iter_per_job']
    num_processing_steps_tr = config_tr['n_iters']      ## level of message-passing
    print("Maximum iterations per job: {}".format(iter_per_job))

    learning_rate = config_tr['learning_rate']
    optimizer = snt.optimizers.Adam(learning_rate)
    model = get_model(config['model_name'])
 
    # prepare graphs
    with_batch_dim = False
    with_pad = False
    print("Node features: ", config['node_features'])
    print("Edge features: ", config['edge_features'])

    file_names = tf.io.gfile.glob(os.path.join(config['tfrec_dir_local'], config['tfrec_name']))
    n_files = len(file_names)
    n_train = int(0.9*n_files)
    if n_train < 1: n_train = 1

    print("Input file names: ", file_names)
    print("{} input files".format(n_files))
    print("{} training files".format(n_train))

    raw_train_dataset = tf.data.TFRecordDataset(file_names[:n_train])
    training_dataset = raw_train_dataset.map(graph.parse_tfrec_function)
    AUTO = tf.data.experimental.AUTOTUNE
    training_dataset = training_dataset.prefetch(AUTO)

    if n_train == n_files:
        testing_dataset = training_dataset
    else:
        raw_test_dataset = tf.data.TFRecordDataset(file_names[n_train:])
        testing_dataset = raw_test_dataset.map(graph.parse_tfrec_function)
    testing_dataset = testing_dataset.prefetch(AUTO)

    # inputs, targets = doublet_graphs.create_graph(batch_size)
    with_batch_dim = False
    inputs, targets = next(training_dataset.take(1).as_numpy_iterator())
    input_signature = (
        graph.specs_from_graphs_tuple(inputs, with_batch_dim),
        graph.specs_from_graphs_tuple(targets, with_batch_dim)
    )

    # training loss
    if config_tr['real_weight']:
        real_weight = config_tr['real_weight']
        fake_weight = config_tr['fake_weight']
    else:
        real_weight = fake_weight = 1.0

    feature_scales = [1000., 1000., 1000.]

    def create_loss_ops(target_op, output_ops):
        weights = target_op.edges * real_weight + (1 - target_op.edges) * fake_weight
        loss_ops = [
            tf.compat.v1.losses.log_loss(target_op.edges, output_op.edges, weights=weights)
            for output_op in output_ops
        ]
        return tf.stack(loss_ops)

    @functools.partial(tf.function, input_signature=input_signature)
    def update_step(inputs_tr, targets_tr):
        print("Tracing update_step")
        with tf.GradientTape() as tape:
            outputs_tr = model(inputs_tr, num_processing_steps_tr)
            loss_ops_tr = create_loss_ops(targets_tr, outputs_tr)
            loss_op_tr = tf.math.reduce_sum(loss_ops_tr) / tf.constant(num_processing_steps_tr, dtype=tf.float32)

        gradients = tape.gradient(loss_op_tr, model.trainable_variables)
        optimizer.apply(gradients, model.trainable_variables)
        return outputs_tr, loss_op_tr

    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
    ckpt_manager = tf.train.CheckpointManager(checkpoint, directory=output_dir, max_to_keep=5)
    if os.path.exists(os.path.join(output_dir, ckpt_name)):
        print("Loading latest checkpoint")
        status = checkpoint.restore(ckpt_manager.latest_checkpoint)

    logged_iterations = []
    losses_tr = []
    corrects_tr = []
    solveds_tr = []

    out_str  = time.strftime('%d %b %Y %H:%M:%S', time.localtime())
    out_str += '\n'
    out_str += "# (iteration number), T (elapsed seconds), Ltr (training loss), Precision, Recall\n"
    log_name = os.path.join(output_dir, config['log_name'])
    with open(log_name, 'a') as f:
        f.write(out_str)

    start_time = time.time()
    last_log_time = start_time
    ## loop over iterations, each iteration generating a batch of data for training
    iruns = 0
    print("# (iteration number), TD (get graph), TR (TF run)")
    last_iteration = 0
    n_epochs = config_tr['epochs']

    logdir = os.path.join(output_dir, "logdir")
    os.makedirs(logdir)
    summary_writer = tf.summary.create_file_writer(logdir=logdir)

    for epoch in range(n_epochs):

        epoch_loss_avg = tf.keras.metrics.Mean()
        epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()

        total_loss = 0.
        num_batches = 0
        for inputs in training_dataset:

            inputs_tr, targets_tr = inputs
            inputs_tr.replace(nodes=inputs_tr.nodes/feature_scales)

            outputs, loss_value = update_step(inputs_tr, targets_tr)
            epoch_loss_avg(loss_value)
            epoch_accuracy(targets_tr.edges, outputs[-1].edges)

            total_loss += loss_value.numpy()
            num_batches += 1

            the_time = time.time()
            elapsed_since_last_log = the_time - last_log_time
            if elapsed_since_last_log > log_every_seconds:
                # save a checkpoint
                ckpt_manager.save()
                last_log_time = time.time()

        with summary_writer.as_default():
            tf.summary.scalar("epoch_loss_avg", epoch_loss_avg.result())
            tf.summary.scalar("epoch_accuracy", epoch_accuracy.result())

        loss_tr = total_loss/num_batches

        elapsed = time.time() - start_time
        outputs_te_list = []
        targets_te_list = []
        for inputs in testing_dataset.take(10).as_numpy_iterator():
            inputs_te, targets_te = inputs
            inputs_te.replace(nodes=inputs_te.nodes/feature_scales)
            outputs_te, loss_te = update_step(inputs_te, targets_te)
            outputs_te_list.append(outputs_te[-1])
            targets_te_list.append(targets_te)
        outputs_te = utils_tf.concat(outputs_te_list, axis=0)
        targets_te = utils_tf.concat(targets_te_list, axis=0)
        correct_tr, solved_tr = compute_matrics(targets_te, outputs_te)
        out_str = "# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Precision {:.4f}, Recall {:.4f}".format(
            epoch, elapsed, loss_tr, loss_te,
            correct_tr, solved_tr)
        print(out_str)

        training_dataset = training_dataset.shuffle(global_batch_size*2, reshuffle_each_iteration=True)
        testing_dataset = testing_dataset.shuffle(global_batch_size*2, reshuffle_each_iteration=True)