#!/usr/bin/env python
import networkx as nx
import numpy as np

from graph_nets import utils_np, utils_tf

from heptrkx import load_yaml
from heptrkx.nx_graph import utils_plot, utils_data, utils_train, prepare, utils_test
from heptrkx.postprocess import wrangler, analysis, inference
from heptrkx import master

import os
import glob
import argparse
import sys

if __name__ == "__main__":

    parser = argparse.ArgumentParser(description='evalute GNN models')
    add_arg = parser.add_argument
    add_arg('config', help='configuration file for training')
    add_arg('evtid', type=int, help='event ID')
    add_arg('outname', help='output plot name')
    add_arg('--iteration',  type=int, default=-1)
    add_arg('--torch', action='store_true')
    add_arg('--ckpt', default=None)

    args = parser.parse_args()
    evtid = args.evtid
    isec = -1
    iteration = args.iteration
    input_ckpt = args.ckpt
    outname = args.outname

    if args.torch:
        from heptrkx.postprocess.evaluate_torch import create_evaluator
    else:
        from heptrkx.postprocess.evaluate_tf import create_evaluator


    config_file = args.config
    config = load_yaml(config_file)
    file_dir = config['make_graph']['out_graph']
    hits_graph_dir = config['data']['input_hitsgraph_dir']
    trk_dir = config['track_ml']['dir']
    if input_ckpt is None:
        input_ckpt = os.path.join(config['segment_training']['output_dir'],
                                  config['segment_training']['prod_name'])

    base_dir =  os.path.join(file_dir, "event{:09d}_g{:09d}_INPUT.npz")
    true_features = ['pt', 'particle_id', 'nhits']
    batch_size = config['segment_training']['parameters']['batch_size']


    file_names = []
    if isec < 0:
        section_patten = base_dir.format(evtid, 0).replace('_g{:09}'.format(0), '*')
        print(section_patten)
        n_sections = int(len(glob.glob(section_patten)))
        print("Total {} sections".format(n_sections))
        file_names = [(base_dir.format(evtid, ii), ii) for ii in range(n_sections)]
    else:
        file_names = [(base_dir.format(evtid, isec), isec)]

    n_batches = len(file_names)//batch_size if len(file_names)%batch_size==0 else len(file_names)//batch_size + 1
    split_inputs = np.array_split(file_names, n_batches)

    event = master.Event(trk_dir, evtid)
    hits = event.hits
    truth = event.truth
    model = create_evaluator(config_file, iteration, input_ckpt)

    all_graphs = []
    is_digraph = True
    is_bidirection = False
    # evaluate each graph
    for ibatch in range(n_batches):
        ## pad batch_size
        current_files = list(split_inputs[ibatch])
        if len(current_files) < batch_size:
            last_file = current_files[-1]
            current_files += [last_file] *(batch_size-len(current_files))


        input_graphs = []
        target_graphs = []
        for items in current_files:
            file_name = items[0]
            with np.load(file_name) as f:
                input_graphs.append(dict(f.items()))

            with np.load(file_name.replace("INPUT", "TARGET")) as f:
                target_graphs.append(dict(f.items()))

        graphs = model(utils_np.data_dicts_to_graphs_tuple(input_graphs),
                       utils_np.data_dicts_to_graphs_tuple(target_graphs),
                       use_digraph=is_digraph, bidirection=is_bidirection
                      )

        # decorate the graph with truth info
        for ii in range(batch_size):
            idx = int(current_files[ii][1])
            id_name = os.path.join(hits_graph_dir, "event{:09d}_g{:03d}_ID.npz".format(evtid, idx))
            with np.load(id_name) as f:
                hit_ids = f['ID']

            for node in graphs[ii].nodes():
                hit_id = hit_ids[node]
                graphs[ii].node[node]['hit_id'] = hit_id
                graphs[ii].node[node]['info'] = hits[hits['hit_id'] == hit_id][true_features].values

            graphs[ii].graph['info'] = [idx] ## section ID

        all_graphs += graphs

    weights = []
    truths = []
    for G in all_graphs:
        weights += [G.edges[edge]['predict'][0] for edge in G.edges()]
        truths += [G.edges[edge]['solution'][0] for edge in G.edges()]

    weights = np.array(weights)
    truths = np.array(truths)
    utils_plot.plot_metrics(weights, truths, odd_th=0.5, outname=outname)

    all_true_tracks = []
    all_predict_tracks = []
    for G in all_graphs:
        all_true_tracks += wrangler.get_tracks(G, feature_name='solution', with_fit=False)
        all_predict_tracks += wrangler.get_tracks(G, feature_name='predict', with_fit=False)

    true_df = analysis.graphs_to_df(all_true_tracks)
    pred_df = analysis.graphs_to_df(all_predict_tracks)

    th = 0.
    good_pids, bad_pids = analysis.label_particles(pred_df, truth, th, ignore_noise=True)
    good_trks = hits[hits['particle_id'].isin(good_pids)]

    res_pred = analysis.summary_on_prediction(G, good_trks, pred_df)
    res_truth = analysis.summary_on_prediction(G, good_trks, true_df)
    inference.print_info(res_pred)
    inference.print_info(res_truth)
