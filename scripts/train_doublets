#!/usr/bin/env python
"""
Use MLPs to classify doublets/pairs.
Inputs are the pre-doublets
"""


if __name__ == "__main__":
    import os
    import argparse

    import pandas as pd
    import numpy as np
    import time

    from bisect import bisect
    from tensorflow import keras

    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import precision_recall_curve

    from heptrkx.nx_graph import shadow_model
    from heptrkx.dataset.event import layer_pairs
    from heptrkx.utils import load_yaml
    from heptrkx.nx_graph.utils_plot import plot_metrics


    parser = argparse.ArgumentParser(description='Train a NN for each layer pair using pre-doublets')
    add_arg = parser.add_argument
    add_arg('input', help='training inputs')
    add_arg('config', help='training configurations')
    add_arg('pair_idx', nargs='?', type=int, default=0, help='which layer pair')
    add_arg('--resume-train',  action='store_true')
    add_arg('--in-eval', action='store_true')
    add_arg('--truth-var', default='solution', help='true variable')
    args = parser.parse_args()

    config = load_yaml(args.config)
    train_cfg = config['doublet_training']

    batch_size = train_cfg['batch_size']
    epochs = train_cfg['epochs']
    pair_idx = args.pair_idx
    output_dir = os.path.join(train_cfg['model_output_dir'], "pair{:03d}".format(pair_idx))
    model_name = train_cfg['model']
    file_name = args.input
    print("Training File Name: {}".format(file_name))

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    ## save checkpoints
    checkpoint_path = os.path.join(output_dir, "weights.hdf5")
    print("Trainde Model will be saved to {}".format(checkpoint_path))

    pair_info = layer_pairs[pair_idx]
    outname = os.path.join(output_dir, 'info{:03d}-{}-{}.txt'.format(pair_idx, *pair_info))
    if os.path.exists(checkpoint_path) \
       and not args.resume_train \
       and os.path.exists(outname):
        print("model is trained and evaluated")
        exit()

    model = getattr(shadow_model, model_name)() 
    model.compile(optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy'])
    if os.path.exists(checkpoint_path) and (args.resume_train or args.in_eval):
        print("Resume previous training")
        model.load_weights(checkpoint_path)

    print("Model is compiled. Start to read training data")
    truth_var = 'solution'
    features = train_cfg['features']
    all_var = features + ['solution']
    with pd.HDFStore(file_name, mode='r') as store:
        t1 = time.time()
        df_input = store['data']
        t2 = time.time()
        df_input = df_input[all_var]
        t3 = time.time()
        print("Time to read background data: {:.0f} seconds".format(t2-t1))
        print("Time to convert format: {:.0f} seconds".format(t3-t2))


    all_inputs  = df_input[features].values
    all_targets = df_input[[truth_var]].values
    n_total = all_inputs.shape[0]
    n_true = np.sum(all_targets)
    n_fake = n_total - n_true
    print("All Entries: {:,}".format(n_total))
    print("True: {:,}".format(n_true))
    print("Fake: {:,}".format(n_fake))

    n_training = int(n_total*0.8)
    n_validating = int(n_total*0.1)

    # transform all inputs
    scaler = StandardScaler()
    all_inputs_normed = scaler.fit_transform(all_inputs)

    inputs = all_inputs_normed[:n_training, :]
    targets = all_targets[:n_training, :]

    x_val = all_inputs_normed[n_training:n_training+n_validating, :]
    y_val = all_targets[n_training:n_training+n_validating, :]
    x_test = all_inputs_normed[n_training+n_validating:, :]
    y_test = all_targets[n_training+n_validating:, :]

    perf_metric = 'val_accuracy'
    if not args.in_eval:
        early_stop = keras.callbacks.EarlyStopping(monitor=perf_metric, min_delta=0.0001)
        check_pt = keras.callbacks.ModelCheckpoint(checkpoint_path, 
                                                monitor=perf_metric, 
                                                save_best_only=True,)
        history = model.fit(inputs, targets,
                        epochs=epochs, batch_size=batch_size,
                        validation_data=(x_val, y_val),
                        callbacks = [early_stop, check_pt],
                        class_weight={0: 1, 1: n_fake/n_true},
                        verbose=1)

    prediction = model.predict(x_test,
                               batch_size=batch_size)

    test_inputs = df_input[n_training+n_validating:]
    test_inputs = test_inputs.assign(prediction=prediction)

    plot_metrics(prediction, y_test,
                 outname=os.path.join(output_dir, 'roc{:03d}_{}-{}.pdf'.format(pair_idx, *pair_info)),
                 off_interactive=True)

    # find a threshold
    y_true = y_test > 0.5
    purity, efficiency, thresholds = precision_recall_curve(y_true, prediction)

    eff_cut = train_cfg['eff_cut']
    ti = bisect(list(reversed(efficiency.tolist())), eff_cut)
    ti = len(efficiency) - ti
    thres = thresholds[ti]
    out = "{} {} {} {th:.4f} {tp:.4f} {fp:.4f} {true} {fake}\n".format(
        pair_idx, *pair_info, th=thres, tp=efficiency[ti], fp=purity[ti],
        true=n_true, fake=n_fake)

    with open(outname, 'a') as f:
        f.write(out)

    # save the model
    # keras.models.save_model(model, checkpoint_path, overwrite=True)