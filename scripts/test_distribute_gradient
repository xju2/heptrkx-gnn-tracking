#!/usr/bin/env python

import tensorflow as tf
import sonnet as snt
import numpy as np

from graph_nets import utils_tf
from graph_nets import blocks
from graph_nets import _base

from heptrkx.dataset import graph as graph_helper
from heptrkx.nx_graph.model import make_mlp_model

print("TensorFlow version:", tf.__version__)

received_edges_reducer = tf.math.unsorted_segment_sum

class NodeBlock(_base.AbstractModule):
    def __init__(self, node_model_fn, name='node_block'):
        super(NodeBlock, self).__init__(name=name)
        with self._enter_variable_scope():
            self._received_edges_aggregator = blocks.ReceivedEdgesToNodesAggregator(received_edges_reducer)
            self._node_model = node_model_fn()

    def _build(self, graph):
        nodes_to_collect = [graph.nodes]
        nodes_to_collect.append(self._received_edges_aggregator(graph))
        collected_nodes = tf.concat(nodes_to_collect, axis=-1)
        updated_nodes = self._node_model(collected_nodes)
        return graph.replace(nodes=updated_nodes)

class EdgeBlock(_base.AbstractModule):
    def __init__(self, edge_model_fn, name='edge_block'):
        super(EdgeBlock, self).__init__(name=name)
        with self._enter_variable_scope():
            self._edge_model = edge_model_fn()
    
    def _build(self, graph):
        replica_ctx = tf.distribute.get_replica_context()
        print('***inside EdgeBlock, show device', replica_ctx.devices)
        agg_node_features = blocks.broadcast_receiver_nodes_to_edges(graph)
        print("***before aggregation:", agg_node_features)
        agg_node_features = replica_ctx.all_reduce("sum", agg_node_features)
        print("***after aggregation:", agg_node_features)

        edges_to_collect = [graph.edges]
        edges_to_collect.append(blocks.broadcast_receiver_nodes_to_edges(graph))
        collected_edges = tf.concat(edges_to_collect, axis=-1)
        updated_edges = self._edge_model(collected_edges)
        return graph.replace(edges=updated_edges)

def test():
    physical_gpus = tf.config.experimental.list_physical_devices("GPU")
    n_gpus = len(physical_gpus)
    print(n_gpus, "GPUs")
    zeros = np.array([0.0], dtype=np.float32)

    graph_dict = {
        'senders': np.array([0, 0, 0, 1, 1, 2, 2, 4, 4, 5, 5, 6, 7, 6]),
        'receivers': np.array([3, 4, 5, 5, 6, 6, 7, 8, 9, 9, 10, 10, 10, 9]),
        "n_edge": 14,
        "n_node": 11,
        'nodes': np.array([[0.1], [0.1], [0.1], [0.2], [0.2], [0.2], [0.2], [0.2], [0.3], [0.3], [0.3]]),
        'edges': np.array([[0.1], [0.8], [0.3], [0.9], [0.6], [0.85], [0.5],[0.9], [0.5], [0.95], [0.45], [0.88], [0.3], [0.3]]),
        'globals': zeros
    }
    graph_dict0 = {
        'senders': np.array([0, 0, 0, 1, 1, 2, 2]),
        'receivers': np.array([3, 4, 5, 5, 6, 6, 7]),
        "n_edge": 7,
        "n_node": 11,
        'nodes': np.array([[0.1], [0.1], [0.1], [0.2], [0.2], [0.2], [0.2], [0.2], [0.3], [0.3], [0.3]]),
        'edges': np.array([[0.1], [0.8], [0.3], [0.9], [0.6], [0.85], [0.5]]),
        'globals': zeros
    }
    graph_dict1 = {
        'senders': np.array([4, 4, 5, 5, 6, 7, 6]),
        'receivers': np.array([8, 9, 9, 10, 10, 10, 9]),
        "n_edge": 7,
        "n_node": 11,
        'nodes': np.array([[0.1], [0.1], [0.1], [0.2], [0.2], [0.2], [0.2], [0.2], [0.3], [0.3], [0.3]]),
        'edges': np.array([[0.9], [0.5], [0.95], [0.45], [0.88], [0.3], [0.3]]),
        'globals': zeros
    }

    target_dict = {
        'senders': np.array([0, 0, 0, 1, 1, 2, 2, 4, 4, 5, 5, 6, 7, 6]),
        'receivers': np.array([3, 4, 5, 5, 6, 6, 7, 8, 9, 9, 10, 10, 10, 9]),
        "n_edge": 14,
        "n_node": 11,
        'nodes': np.array([[0.1], [0.1], [0.1], [0.2], [0.2], [0.2], [0.2], [0.2], [0.3], [0.3], [0.3]]),
        'edges': np.array([[0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [1], [0], [0]]),
        'globals': zeros
    }
    target_dict0 = {
        'senders': np.array([0, 0, 0, 1, 1, 2, 2]),
        'receivers': np.array([3, 4, 5, 5, 6, 6, 7]),
        "n_edge": 7,
        "n_node": 11,
        'nodes': np.array([[0.1], [0.1], [0.1], [0.2], [0.2], [0.2], [0.2], [0.2], [0.3], [0.3], [0.3]]),
        'edges': np.array([[0], [1], [0], [1], [0], [1], [0]]),
        'globals': zeros
    }
    target_dict1 = {
        'senders': np.array([4, 4, 5, 5, 6, 7, 6]),
        'receivers': np.array([8, 9, 9, 10, 10, 10, 9]),
        "n_edge": 7,
        "n_node": 11,
        'nodes': np.array([[0.1], [0.1], [0.1], [0.2], [0.2], [0.2], [0.2], [0.2], [0.3], [0.3], [0.3]]),
        'edges': np.array([[1], [0], [1], [0], [1], [0], [0]]),
        'globals': zeros
    }


    tot_graph = utils_tf.data_dicts_to_graphs_tuple([graph_dict])
    g0 = utils_tf.data_dicts_to_graphs_tuple([graph_dict0])
    g1 = utils_tf.data_dicts_to_graphs_tuple([graph_dict1])

    t0 = utils_tf.data_dicts_to_graphs_tuple([target_dict0])
    t1 = utils_tf.data_dicts_to_graphs_tuple([target_dict1])
    dtypes, shapes = graph_helper.dtype_shape_from_graphs_tuple(g0, with_padding=False, debug=False)
    t_dtypes, t_shapes = graph_helper.dtype_shape_from_graphs_tuple(t0, with_padding=False, debug=False)

    def generator():
        for g in [(g0, t0), (g1, t1)]:
            yield g

    dataset = tf.data.Dataset.from_generator(generator, output_types=(dtypes, t_dtypes), output_shapes=(shapes, t_shapes))
    dataset = dataset.batch(2)
    # print(dataset)

    ## edges to nodes
    e2n_0 = received_edges_reducer(g0.edges, g0.receivers, tf.reduce_sum(g0.n_node))
    e2n_1 = received_edges_reducer(g1.edges, g1.receivers, tf.reduce_sum(g1.n_node))
    e2n = received_edges_reducer(tot_graph.edges, tot_graph.receivers, tf.reduce_sum(tot_graph.n_node))
    print(e2n_0)
    print(e2n_1)
    # print(e2n_0 + e2n_1)
    print(e2n)
    # exit(0)

    strategy = snt.distribute.Replicator(['/device:GPU:{}'.format(i) for i in range(n_gpus)],\
        tf.distribute.ReductionToOneDevice("GPU:0"))
    
    with strategy.scope():
        # node_block = blocks.NodeBlock(make_mlp_model, use_globals=False)
        # aggregator = blocks.ReceivedEdgesToNodesAggregator(tf.math.unsorted_segment_sum)
        edge_fn = lambda: snt.Sequential([
            snt.nets.MLP([2, 1], activation=tf.nn.relu, name='edge_output'),
            tf.sigmoid
        ])
        node_block = NodeBlock(node_model_fn=make_mlp_model)
        edge_block = EdgeBlock(edge_model_fn=edge_fn)


    dist_dataset = strategy.experimental_distribute_dataset(dataset)


    def create_loss_ops(target_op, output_ops):
        loss_ops = [
            tf.compat.v1.losses.log_loss(target_op.edges, output_op.edges)
            for output_op in output_ops]
        return tf.stack(loss_ops)

    # @tf.function
    def update_step(input_tr, target_tr):
        print("Tracing update step")
        # print(input_tr)
        input_tr = graph_helper.concat_batch_dim(input_tr)
        target_tr = graph_helper.concat_batch_dim(target_tr)
        with tf.GradientTape() as tape:
            output = edge_block(input_tr)
            loss = tf.compat.v1.losses.log_loss(target_tr.edges, output.edges)
            loss = tf.math.reduce_sum(loss)

        gradients = tape.gradient(loss, edge_block.trainable_variables)
        print("---Before aggrepation:", gradients)
        replica_ctx = tf.distribute.get_replica_context()
        print('device', replica_ctx.devices)
        print('num_replicas', replica_ctx.num_replicas_in_sync)
        print('replica id', replica_ctx.replica_id_in_sync_group)

        gradients = replica_ctx.all_reduce("mean", gradients)
        print("===After aggregation:", gradients)
        # optimizer.apply(gradients, edge_block.trainable_variables)
        return output

    for inputs in dist_dataset:
        input_tr, target_tr = inputs
        agg = strategy.run(update_step, args=(input_tr, target_tr))

if __name__ == "__main__":
    test()