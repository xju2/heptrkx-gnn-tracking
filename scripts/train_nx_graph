#!/usr/bin/env python
"""
Training GNN
"""

import tensorflow as tf

import os
import sys
import argparse
import glob
import re
import time

import numpy as np
import sklearn.metrics


from graph_nets import utils_tf
from graph_nets import utils_np
import sonnet as snt

from heptrkx.dataset.graph import DoubletGraphGenerator
# from heptrkx.nx_graph import utils_train
# from heptrkx.nx_graph import prepare
from heptrkx.nx_graph import get_model
from heptrkx.utils import load_yaml
# ckpt_name = 'checkpoint_{:05d}.ckpt'
ckpt_name = 'checkpoint'

prog_name = os.path.basename(sys.argv[0])

def eval_output(target, output):
    """
    target, output are graph-tuple from TF-GNN,
    each of them contains N=batch-size graphs
    """
    tdds = utils_np.graphs_tuple_to_data_dicts(target)
    odds = utils_np.graphs_tuple_to_data_dicts(output)

    test_target = []
    test_pred = []
    for td, od in zip(tdds, odds):
        test_target.append(td['edges'])
        test_pred.append(np.squeeze(od['edges']))

    test_target = np.concatenate(test_target, axis=0)
    test_pred   = np.concatenate(test_pred,   axis=0)
    return test_pred, test_target


def compute_matrics(target, output, thresh=0.5):
    test_pred, test_target = eval_output(target, output)
    y_pred, y_true = (test_pred > thresh), (test_target > thresh)
    return sklearn.metrics.precision_score(y_true, y_pred), sklearn.metrics.recall_score(y_true, y_pred)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Train nx-graph with configurations')
    add_arg = parser.add_argument
    add_arg('config', help='configuration file')
    args = parser.parse_args()

    all_config = load_yaml(args.config)
    config = all_config['gnn_training']


    # add ops to save and restore all the variables
    prod_name = config['prod_name']
    output_dir = os.path.join(config['output_dir'], prod_name)
    print("[{}] save models at {}".format(prog_name, output_dir))
    os.makedirs(output_dir, exist_ok=True)

    # files = glob.glob(output_dir+"/*.ckpt.meta")
    # last_iteration = 0 if len(files) < 1 else max([
    #     int(re.search('checkpoint_([0-9]*).ckpt.meta', os.path.basename(x)).group(1))
    #     for x in files
    # ])
    # print("[{}] last iteration: {}".format(prog_name, last_iteration))

    config_tr = config['parameters']
    # How much time between logging and printing the current results.
    # save checkpoint very 10 mins
    log_every_seconds       = config_tr['time_lapse']
    batch_size = n_graphs   = config_tr['batch_size']   # need optimization
    num_training_iterations = config_tr['iterations']
    iter_per_job            = 2500 if 'iter_per_job' not in config_tr else config_tr['iter_per_job']
    num_processing_steps_tr = config_tr['n_iters']      ## level of message-passing
    print("Maximum iterations per job: {}".format(iter_per_job))

    print("Node features: ", config['node_features'])
    print("Edge features: ", config['edge_features'])
    doublet_graphs = DoubletGraphGenerator(
        config['n_eta'], config['n_phi'],
        config['node_features'], config['edge_features']
        )
    for hit_file, doublet_file in zip(config['hit_files'], config['doublet_files']):
        doublet_graphs.add_file(hit_file, doublet_file)

    learning_rate = config_tr['learning_rate']
    optimizer = snt.optimizers.Adam(learning_rate)
    model = get_model(config['model_name'])

    # training loss
    loss_weights = 1.0
    if config_tr['real_weight']:
        real_weight = config_tr['real_weight']
        fake_weight = config_tr['fake_weight']
    else:
        real_weight = fake_weight = 1.0

    def create_loss_ops(target_op, output_ops):
        # only use edges
        weights = target_op.edges * real_weight + (1 - target_op.edges) * fake_weight
        # print(output_ops[0].edges.shape)
        # print(target_op.edges.shape)
        loss_ops = [
            tf.compat.v1.losses.log_loss(target_op.edges, tf.squeeze(output_op.edges))
            for output_op in output_ops
        ]
        return loss_ops

    def update_step(inputs_tr, targets_tr):
        with tf.GradientTape() as tape:
            outputs_tr = model(inputs_tr, num_processing_steps_tr)
            loss_ops_tr = create_loss_ops(targets_tr, outputs_tr)
            loss_op_tr = sum(loss_ops_tr) / num_processing_steps_tr

        gradients = tape.gradient(loss_op_tr, model.trainable_variables)
        optimizer.apply(gradients, model.trainable_variables)
        return outputs_tr, loss_op_tr

    @tf.function
    def get_data():
        in_graphs, out_graphs = doublet_graphs.create_graph(batch_size, is_training=True)
        in_graphs = utils_tf.set_zero_global_features(in_graphs, 1)
        out_graphs = utils_tf.set_zero_global_features(out_graphs, 1)
        out_graphs = utils_tf.set_zero_node_features(out_graphs, 1)
        return in_graphs, out_graphs

    @tf.function
    def get_test_data():
        in_graphs, out_graphs = doublet_graphs.create_graph(batch_size, is_training=False)
        in_graphs = utils_tf.set_zero_global_features(in_graphs, 1)
        out_graphs = utils_tf.set_zero_global_features(out_graphs, 1)
        out_graphs = utils_tf.set_zero_node_features(out_graphs, 1)
        return in_graphs, out_graphs

    # Get some example data that resembles the tensors that will be fed
    # into update_step():
    example_input_data, example_target_data = get_data()
    # Get the input signature for that function by obtaining the specs
    input_signature = [
        utils_tf.specs_from_graphs_tuple(example_input_data),
        utils_tf.specs_from_graphs_tuple(example_target_data)
    ]
    # Compile the update function using the input signature for speedy code.
    compiled_update_step = tf.function(update_step, input_signature=input_signature)

    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
    if os.path.exists(os.path.join(output_dir, ckpt_name)):
        # ckpt_path = os.path.join(output_dir, ckpt_name.format(last_iteration))
        # print("loading checkpoint:", ckpt_path)
        print("Loading latest checkpoint")
        status = checkpoint.restore(tf.train.latest_checkpoint(output_dir))

    logged_iterations = []
    losses_tr = []
    corrects_tr = []
    solveds_tr = []

    out_str  = time.strftime('%d %b %Y %H:%M:%S', time.localtime())
    out_str += '\n'
    out_str += "# (iteration number), T (elapsed seconds), Ltr (training loss), Precision, Recall\n"
    log_name = os.path.join(output_dir, config['log_name'])
    with open(log_name, 'a') as f:
        f.write(out_str)

    start_time = time.time()
    last_log_time = start_time
    ## loop over iterations, each iteration generating a batch of data for training
    iruns = 0

    print("# (iteration number), TD (get graph), TR (TF run)")
    last_iteration = 0
    for iteration in range(last_iteration, num_training_iterations):
        if iruns > iter_per_job:
            print("runs larger than {} iterations per job, stop".format(iter_per_job))
            break
        else: iruns += 1
        last_iteration = iteration

        inputs_tr, targets_tr = get_data()
        # print(inputs_tr.n_node, inputs_tr.n_edge)
        # print(inputs_tr.nodes, inputs_tr.edges)
        outputs_tr, loss_tr = update_step(inputs_tr, targets_tr)
        # outputs_tr, loss_tr = compiled_update_step(inputs_tr, targets_tr)

        the_time = time.time()
        elapsed_since_last_log = the_time - last_log_time

        if elapsed_since_last_log > log_every_seconds:
            # save a checkpoint
            # ckpt_path = os.path.join(output_dir, ckpt_name.format(last_iteration))
            ckpt_path = os.path.join(output_dir, ckpt_name+".ckpt")
            checkpoint.save(file_prefix=ckpt_path)

            last_log_time = the_time
            inputs_te, targets_te = get_test_data()
            # outputs_te, loss_te = compiled_update_step(inputs_te, targets_te)
            outputs_te, loss_te = update_step(inputs_te, targets_te)
            correct_tr, solved_tr = compute_matrics(targets_te, outputs_te[-1])

            elapsed = time.time() - start_time
            logged_iterations.append(iteration)
            out_str = "# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Precision {:.4f}, Recall {:.4f}".format(
                iteration, elapsed, loss_tr, loss_te,
                correct_tr, solved_tr)
            print(out_str)