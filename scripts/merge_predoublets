#!/usr/bin/env python 
"""
Merge predoublets for training.
The idea is to create true predoublets from 1000 events
and fake predoublets from 2 events,
merge into one file for training doublets
"""


if __name__ == "__main__":
    import argparse
    import time
    import os

    from heptrkx.utils import is_df_there
    import numpy as np
    import pandas as pd
    from sklearn.utils import shuffle

    parser = argparse.ArgumentParser(description="merge true and fake pairs")
    add_arg = parser.add_argument
    add_arg('file_names', nargs='+', help='file names')
    add_arg('--outdir', help='output directory', default='.')
    add_arg('--layer', type=int, help='layer id', default=None)
    args = parser.parse_args()

    file_names = args.file_names
    outdir = args.outdir

    file_name = file_names[0]
    with pd.HDFStore(file_name) as store:
        evtid_list = np.unique([int(x.split('/')[1][3:]) for x in store.keys()]).tolist()
        pair_list = np.unique([int(x.split('/')[2][4:]) for x in store.keys()]).tolist()

    print("{} events and {} pairs".format(len(evtid_list), len(pair_list)))
    if args.layer:
        pair_list = [args.layer]

    for pair in pair_list:
        df_list = []
        outname = os.path.join(outdir, 'pairs4TrainingNN_pair{:03}.h5'.format(pair))
        if is_df_there(outname):
            continue

        for file_name in file_names:
            with pd.HDFStore(file_name, mode='r') as store:
                for evtid in evtid_list:
                    key_name = "evt{}/pair{}".format(evtid, pair)
                    df_list.append(store[key_name])

        df_all = pd.concat(df_list, ignore_index=True)
        df_all = shuffle(df_all, randome_state=10)
        n_true = df_all[df_all.solution]
        n_fake = df_all[~df_all.solution]
        print("Pair {} has {} true and {} fake".format(pair, n_true, n_fake))

        with pd.HDFStore(outname, mode='w') as store:
            store['data'] = df_all