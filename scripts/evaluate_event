#!/usr/bin/env python
"""
Use GNN to evaluate one event
"""

import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt

from heptrkx import master, seeding
from heptrkx import load_yaml, layer_pairs, select_pair_layers, pairwise
from heptrkx.nx_graph import utils_data
from heptrkx.postprocess import wrangler, analysis, inference
from heptrkx.preprocess import hitsgraph


if __name__ == "__main__":
    import argparse
    import sys

    parser = argparse.ArgumentParser(description='Use GNN model to calculate efficiency and purity for one event')
    add_arg = parser.add_argument
    add_arg('config', help="configuration file")
    add_arg('evtid', type=int, help="event id")
    add_arg('--min-hits', type=int, default=1, help="minimum number of hits")
    args = parser.parse_args()

    config_file = args.config
    evtid = args.evtid
    min_hits = args.min_hits

    # read configuration, extract used parameters
    config = load_yaml(config_file)
    data_dir = config['track_ml']['dir']
    layers = config['track_ml']['layers']
    print("Study Event {}".format(evtid))
    print("Use hits recorded in Layers:", layers)
    z0_max = config['selection']['z0_max']
    phi_slope_max = config['selection']['phi_slope_max']
    n_phi_sections = config['selection']['n_phi_sections']
    n_eta_sections = config['selection']['n_eta_sections']
    eta_range = (-5, 5)
    phi_range = (-np.pi, np.pi)

    try:
        black_list_dir = config['track_ml']['blacklist_dir']
    except KeyError:
        black_list_dir = None

    # read in the event
    event = master.Event(data_dir, evtid, black_list_dir)
    particles = event.particles
    truth = event.truth

    n_tot_particles = particles.shape[0]
    n_tot_hits = event.hits.shape[0]
    print("Total {:,} hits, {:,} particles".format(
        n_tot_hits, n_tot_particles))

    reconstructable_particles = event.reconstructable_pids(min_hits=min_hits)
    n_reco_trks = len(reconstructable_particles)
    print("{:,} ({:.2f}%) reconstructable particles".format(n_reco_trks, n_reco_trks*100/n_tot_particles))

    hits_no_noise = event.remove_noise_hits()
    n_nonnoise_hits = hits_no_noise.shape[0]
    n_noise_hits = n_tot_hits - n_nonnoise_hits
    print("{:,} ({:.2f}%) noise hits, {} non-noise hits".format(
        n_noise_hits, n_noise_hits*100/n_tot_hits, n_nonnoise_hits))

    hits_barrel = event.filter_hits(layers)
    n_hits = hits_barrel.shape[0]
    print("{:,} non-noise hits in selected layers, "
          "{:.2f}% of total non-noise hits".format(
        n_hits, n_hits*100/n_nonnoise_hits))

    hits_barrel_no_dup = event.remove_duplicated_hits()
    n_nodes = hits_barrel_no_dup.shape[0]
    n_duplicated_hits = n_hits - n_nodes
    print("{:,} non-noise non-duplication hits left, "
          "{:.2f}% of total non-noise hits".format(
        n_nodes, n_nodes*100/n_nonnoise_hits))
    print("{:,} duplicated hits, "
          "{:.2f}% of total non-noise hits".format(
              n_duplicated_hits, n_duplicated_hits*100/n_nonnoise_hits))
    ## summarize number of hits and number of particles with min_hits
    good_particles = hits_barrel_no_dup.groupby('particle_id')['hit_id'].count() >= min_hits
    n_good_particles = np.sum(good_particles)
    print("Summary: {:,} particles "
          "with minimum {} hit(s) and {:,} hits".format(
              n_good_particles, min_hits, n_nodes))

    ## create doublets
    selected_pairs_idx = select_pair_layers(layers)
    selected_pairs = [layer_pairs[i] for i in selected_pairs_idx]
    segments = []
    for pair_idx in selected_pairs_idx:
        segments.append(
            seeding.create_segments(hits_barrel_no_dup,
                                    layer_pairs[pair_idx],
                                    cluster_info=False,
                                    origin_pos=False,
                                    verbose=False))

    pd_segments = pd.concat(segments, ignore_index=True)
    true_edges = pd_segments[pd_segments['true']]
    selected_segments = pd_segments[ (pd_segments.phi_slope.abs() < phi_slope_max)
                                    & (pd_segments.z0.abs() < z0_max) ]
    selected_true_segments = selected_segments[selected_segments['true']]
    n_seg_truth = true_edges.shape[0]
    n_seg_sel = selected_segments.shape[0]
    n_seg_sel_true = selected_true_segments.shape[0]
    print("Segment selection\n"
          "\tEfficiency: {:.2f}%\n"
          "\tPurity: {:.2f}%".format(
              100*n_seg_sel_true/n_seg_truth,
              100*n_seg_sel_true/n_seg_sel
          ))

    ## split selected segments into [eta, phi] regions
    phi_edges = np.linspace(*phi_range, num=n_phi_sections+1)
    eta_edges = np.linspace(*eta_range, num=n_eta_sections+1)
    hits_sections = hitsgraph.split_detector_sections(
        hits_barrel_no_dup, phi_edges, eta_edges)

    # construct hits graph
    feature_scale = np.array([1000., np.pi / n_phi_sections, 1000.])
    feature_names = ['r', 'phi', 'z']
    # hitsgraphs = [hitsgraph.construct_graph(section_hits, selected_pairs,
    #                                         phi_slope_max=phi_slope_max, z0_max=z0_max,
    #                                         feature_names=feature_names,
    #                                         feature_scale=feature_scale)
    #               for section_hits in hits_sections]

    #now recreate segments from the splitted graphs
    splitted_segments = [[seeding.create_segments(section_hits, layer_pair,
                                                  cluster_info=False,
                                                  origin_pos=False,
                                                  verbose=False)
                          for layer_pair in selected_pairs]
                         for section_hits in hits_sections]
    merged_splitted_segments = pd.concat([item for sublist in splitted_segments for item in sublist], ignore_index=True)
    sp_true_edges = merged_splitted_segments[merged_splitted_segments['true']]
    sp_sel_edges = merged_splitted_segments[
        (merged_splitted_segments.phi_slope.abs() < phi_slope_max)
        &(merged_splitted_segments.z0.abs() < z0_max)]
    sp_sel_true_edges = sp_sel_edges[sp_sel_edges['true']]
    n_seg_sp_truth = sp_true_edges.shape[0]
    n_seg_sp_sel = sp_sel_edges.shape[0]
    n_seg_sp_sel_true = sp_sel_true_edges.shape[0]
    print("Segment selection after spliting into {} regions\n"
          "\tEfficiency: {:.2f}%\n"
          "\tPurity: {:.2f}%".format(
              n_phi_sections*n_eta_sections,
              100*n_seg_sp_sel_true/n_seg_truth,
              100*n_seg_sp_sel_true/n_seg_sp_sel
          ))

    # construct netowrkx graph and make predictions
    nx_G_splitted = utils_data.segments_to_nx(hits_barrel_no_dup,
                                              sp_sel_true_edges,
                                              'hit_id_in', 'hit_id_out',
                                              'true', use_digraph=False)
    results_on_truth = inference.get_corrected_trks(nx_G_splitted, truth,
                                                    hits_barrel_no_dup)


    inference.print_info(results_on_truth)
