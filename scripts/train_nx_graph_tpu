#!/usr/bin/env python
"""
Training GNN in TPU
"""

import tensorflow as tf
print(tf.__version__)

import os
import sys
import argparse
import glob
import re
import time
import random
import functools

import numpy as np
import sklearn.metrics


from graph_nets import utils_tf
from graph_nets import utils_np
import sonnet as snt

from heptrkx.dataset import graph
from heptrkx.nx_graph import get_model
from heptrkx.utils import load_yaml

prog_name = os.path.basename(sys.argv[0])

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Train nx-graph with configurations')
    add_arg = parser.add_argument
    add_arg('config', help='configuration file')
    add_arg('--tpu', help='use tpu', default='colab')
    args = parser.parse_args()

    print(args.config)
    all_config = load_yaml(args.config)
    config = all_config['gnn_training']
    with_batch_dim = False
    with_pad = True

    # first start the tpu
    if args.tpu == 'colab':
        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
    else:
        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=args.tpu)
    tf.config.experimental_connect_to_cluster(resolver)
    tf.tpu.experimental.initialize_tpu_system(resolver)
    strategy = snt.distribute.TpuReplicator(resolver)

    # add ops to save and restore all the variables
    prod_name = config['prod_name']
    output_dir = os.path.join(config['output_dir'], prod_name)
    print("[{}] save models at {}".format(prog_name, output_dir))
    # os.makedirs(output_dir, exist_ok=True)

    config_tr = config['parameters']
    global_batch_size = n_graphs   = config_tr['batch_size']   # need optimization
    num_processing_steps_tr = config_tr['n_iters']      ## level of message-passing
    n_epochs = config_tr['epochs']
    print("{} epochs".format(n_epochs))
    print("{} processing steps in the model".format(num_processing_steps_tr))
    # prepare graphs
    print("Node features: ", config['node_features'])
    print("Edge features: ", config['edge_features'])

    # FIXME: add as a argument. <xju>
    file_names = tf.io.gfile.glob(os.path.join(config['tfrec_dir'], config['tfrec_name']+"*"))
    print("Input file names: ", file_names)
    raw_dataset = tf.data.TFRecordDataset(file_names)
    training_dataset = raw_dataset.map(graph.parse_tfrec_function)

    AUTO = tf.data.experimental.AUTOTUNE
    training_dataset = training_dataset.repeat().shuffle(2048).batch(global_batch_size).prefetch(AUTO)
    def get_dataset_iterator(dataset, n_examples):
        return dataset.unbatch().batch(n_examples).as_numpy_iterator()
    
    training_viz_iterator = get_dataset_iterator(training_dataset, 1)
    g1, g2 = next(training_viz_iterator)
    print(g1.edges.shape)

    learning_rate = config_tr['learning_rate']
    with strategy.scope():
        optimizer = snt.optimizers.Adam(learning_rate)
        model = get_model(config['model_name'])

    # output_dir = 'gs://gnn-v1/model'
    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
    ckpt_manager = tf.train.CheckpointManager(checkpoint, directory=output_dir, max_to_keep=5)
    print("Loading latest checkpoint")
    status = checkpoint.restore(ckpt_manager.latest_checkpoint)

    # training loss
    if config_tr['real_weight']:
        real_weight = config_tr['real_weight']
        fake_weight = config_tr['fake_weight']
    else:
        real_weight = fake_weight = 1.0

    # FIXME: remove padding graphs from loss function. <xju>
    def create_loss_ops(target_op, output_ops):
        weights = target_op.edges * real_weight + (1 - target_op.edges) * fake_weight
        # row_index = tf.range(tf.math.reduce_sum(target_op.n_edge))
        # n_valid_edges = target_op.n_edge[0]
        # mask = tf.cast(row_index < n_valid_edges, tf.float64)
        # mask = tf.expand_dims(mask, axis=1)
        # weights = weights * mask
        loss_ops = [
            # tf.compat.v1.losses.log_loss(target_op.edges, output_op.edges, weights=weights)
            tf.nn.weighted_cross_entropy_with_logits(target_op.edges, output_op.edges, pos_weight=real_weight)
            for output_op in output_ops
        ]
        return tf.stack(loss_ops)

    @tf.function(autograph=False)
    def train_step(inputs_tr, targets_tr):
        print("Tracing train_step")
        print(inputs_tr)
        
        def update_step(inputs_tr, targets_tr):
            print("Tracing update_step")
            inputs_tr = graph.concat_batch_dim(inputs_tr)
            targets_tr = graph.concat_batch_dim(targets_tr)
            with tf.GradientTape() as tape:
                outputs_tr = model(inputs_tr, num_processing_steps_tr)
                loss_ops_tr = create_loss_ops(targets_tr, outputs_tr)
                loss_op_tr = tf.math.reduce_sum(loss_ops_tr) / tf.constant(num_processing_steps_tr, dtype=tf.float32)

            gradients = tape.gradient(loss_op_tr, model.trainable_variables)
            # aggregate the gradients from the full batch.
            # this is not there for mirror strategy
            # replica_ctx = tf.distribute.get_replica_context()
            # gradients = replica_ctx.all_reduce("mean", gradients)

            optimizer.apply(gradients, model.trainable_variables)
            return loss_op_tr


        per_example_losses = strategy.run(update_step, args=(inputs_tr,targets_tr))
        mean_loss = strategy.reduce("sum", per_example_losses, axis=None)
        return mean_loss

    def train_epoch(dataset):
        total_loss = 0.
        num_batches = 0
        for inputs in dataset:
            # print("Step {}".format(num_batches))
            input_tr, target_tr = inputs
            # print(target_tr)
            total_loss += train_step(input_tr, target_tr)
            num_batches += 1
        return total_loss/num_batches

    now = time.time()
    for epoch in range(n_epochs):
        print("start epoch {}".format(epoch))
        # training_dataset = training_dataset.shuffle(global_batch_size*2, reshuffle_each_iteration=True)
        # print("after shuffle {}".format(epoch))
        # distributed dataset
        # dist_training_dataset = strategy.experimental_distribute_dataset(training_dataset)
        print("starting training on TPU")
        for inputs in training_dataset:
            input_tr, target_tr = inputs
            tf.print(tf.shape(input_tr.edges))
        # loss = train_epoch(training_dataset)
        this_epoch = time.time()
        print("Training {} epoch, {:.2f} mins, Loss := {}".format(epoch, (this_epoch-now)/60., loss/global_batch_size))
        now = this_epoch
        ckpt_manager.save()