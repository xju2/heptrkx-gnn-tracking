#!/usr/bin/env python
"""
THis reads a hit file and create pre-doublets.
Pre-doublets are the doublets that pass very loose selections,
later used for either making graphs or training doublet classifier.
Pre-doublets are divided into different detector regions.

These selection typically yields 94-96% doublet efficiency.
"""

import argparse
import os
import numpy as np
import pandas as pd
import itertools
import more_itertools

from heptrkx.dataset.doublet import create_segments, calculate_segment_features
from heptrkx.dataset.event import layer_pairs, select_pair_layers, pairs_layer_dict
from heptrkx.utils import list_from_str
from heptrkx.utils import is_df_there

selected_features =  [
    'evtid', 'solution',
    'hit_id_in', 'hit_id_out',
    'dphi', 'dz', 'dr', 'phi_slope', 'z0', 'deta', 'deta1', 'dphi1',
    # 'x_in', 'y_in', 'z_in',
    # 'x_out', 'y_out' 'z_out',
    # 'hit_idx_in', 'hit_idx_out',
    # 'layer_in', 'layer_out',
]
hit_features = [
    'hit_id', 'evtid', 'r', 'phi', 'z', 'eta', 'geta', 'gphi', 'particle_id', 'layer'
]

def process(hit_file_name, evtid, pairids, outname, phi_slope_cut, z0_cut, only_truth=False, verbose=False):
    with pd.HDFStore(outname, mode='w') as store:
        with pd.HDFStore(hit_file_name, mode='r') as store_in:
            hits = store_in[evtid]
            hits = hits[hit_features]
            ipair = 0
            for segment in create_segments(hits, pairids, only_true=only_truth):
                features = calculate_segment_features(segment)
                # fine tunning for barrel region
                # [TODO], need a map
                if ipair < 3:
                    mask = (features['phi_slope'].abs() < phi_slope_cut) & (features['z0'].abs() < z0_cut)
                elif ipair < 7:
                    mask = (features['phi_slope'].abs() < phi_slope_cut) & (features['z0'].abs() < 800)
                else:
                    mask = (features['phi_slope'].abs() < phi_slope_cut) & (features['z0'].abs() < 1000)

                segment = segment.assign(**features)[selected_features]
                del features
                passed_segments = segment[mask]
                # print(passed_segments.columns)

                solution = segment.solution
                true_doublets = passed_segments[passed_segments.solution].shape[0]
                total_true_doublets = solution[solution].shape[0]
                efficiency = true_doublets*100/total_true_doublets
                purity = true_doublets*100/passed_segments.shape[0]
                if verbose:
                    print("Layer {:2} - {:2}, {:10,}, efficiency: {:.2f}, purity: {:.2f}".format(
                        selected_layer_pairs[ipair][0],
                        selected_layer_pairs[ipair][1],
                        passed_segments.shape[0], efficiency, purity))

                key = "{}/pair{}".format(evtid, ipair)
                store[key] = passed_segments
                ipair += 1

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="read hit files and produce pre-doublets")
    add_arg = parser.add_argument
    add_arg('hit_file_name', help='input hit files')
    add_arg('outname', help='output name')
    add_arg('--evtid', default=None, help='process that event')
    add_arg('--input-dir',
            default='/global/cfs/projectdirs/m3443/usr/xju/heptrkx/codalab/inputs/train_all',
            help='input trackML data')
    add_arg('--phislope-cut', type=float, default=0.004)
    add_arg('--z0-cut', type=float, default=500.)
    add_arg('--layers', default="7, 8, 9, 10, 24, 25, 26, 27, 40, 41", help='layers of interest')
    add_arg('--only-truth', action='store_true', help='only save true doublets, used for training')
    add_arg('--mpi', action='store_true', help='use MPI')
    args = parser.parse_args()

    hit_file_name = args.hit_file_name
    outname = args.outname
    evtid = args.evtid
    input_dir = args.input_dir
    phi_slope_cut = args.phislope_cut
    z0_cut = args.z0_cut
    layers = [int(x) for x in args.layers.split(',')]
    only_truth = args.only_truth
    use_mpi = args.mpi

    if is_df_there(outname):
        print("out put is there:", outname)
        exit()
    
    layers_idx = select_pair_layers(layers)
    selected_layer_pairs = [layer_pairs[x] for x in layers_idx]

    rank = 0
    size = 1
    if use_mpi:
        try:
            from mpi4py import MPI
            comm = MPI.COMM_WORLD
            size = comm.Get_size()
            rank = comm.Get_rank()
        except ImportError:
            print("Does not have mpi4py")
            pass

    if rank == 0:
        print("World size:", size, "rank", rank)
        with pd.HDFStore(hit_file_name, mode='r') as hit_file:
            if evtid:
                evtids = ["evt{}".format(x) for x in list_from_str(evtid)]
            else:
                evtids = list(hit_file.keys())
            
            print("Total {} events".format(len(evtids)))
            jobs = [list(j) for j in more_itertools.divide(size, evtids)]
            # jobs = list(itertools.product(evtids, selected_layer_pairs, repeat=1))
            # print("Total {} jobs".format(len(jobs)))
            # jobs = [list(j) for j in more_itertools.divide(size, jobs)]
    else:
        jobs = None

    if use_mpi:
        comm.Barrier()
        jobs = comm.scatter(jobs, root=0)
    else:
        jobs = jobs[0]
    
    tmp_basename = "out_{}.h5"
    print(rank, 'has', len(jobs), 'jobs')

    for evtid in jobs:
        evtid = evtid.replace('/', '')
        process(hit_file_name, evtid,\
            selected_layer_pairs,
            tmp_basename.format(rank),
            phi_slope_cut, z0_cut
            )
    print(rank, 'finished')